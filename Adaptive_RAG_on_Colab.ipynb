{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive RAG on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs an Adaptive RAG system on Google Colab. It's a conversion of the code from the [LangGraph AI repository](https://github.com/piyushagni5/langgraph-ai/tree/main/agentic-rag/agentic-rag-systems/building-adaptive-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q beautifulsoup4 langchain-community tiktoken langchainhub langchain langgraph tavily-python langchain-openai python-dotenv black isort pytest langchain-chroma langchain-tavily==0.1.5 langchain_aws langchain_google_genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Environment Variables\n\nYou need to provide API keys for Google and Tavily.\n\n- **`GOOGLE_API_KEY`**: Your Google API key for Gemini models. You can get one [here](https://aistudio.google.com/app/apikey).\n- **`TAVILY_API_KEY`**: Your Tavily API key for web search. You can get one [here](https://app.tavily.com/).\n- **`LANGCHAIN_API_KEY`** (Optional): Your LangSmith API key for tracing. You can get one [here](https://smith.langchain.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport getpass\n\nos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\nos.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API Key: \")\n\n# Optional: LangSmith for tracing\nuse_langsmith = input(\"Do you want to use LangSmith for tracing? (yes/no): \").lower()\nif use_langsmith == 'yes':\n    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API Key: \")\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n    os.environ[\"LANGCHAIN_PROJECT\"] = \"agentic-rag\"\nelse:\n    if \"LANGCHAIN_API_KEY\" in os.environ:\n        del os.environ[\"LANGCHAIN_API_KEY\"]\n    if \"LANGCHAIN_TRACING_V2\" in os.environ:\n        del os.environ[\"LANGCHAIN_TRACING_V2\"]\n    if \"LANGCHAIN_ENDPOINT\" in os.environ:\n        del os.environ[\"LANGCHAIN_ENDPOINT\"]\n    if \"LANGCHAIN_PROJECT\" in os.environ:\n        del os.environ[\"LANGCHAIN_PROJECT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recreate the Project's File Structure\n\nThis will write the Python files from the original project to the Colab filesystem. This preserves the original, modular structure of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n\nos.makedirs(\"graph/chains\", exist_ok=True)\nos.makedirs(\"graph/nodes\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nllm_model = ChatGoogleGenerativeAI(\n    model=\"gemini-1.5-flash\",\n    temperature=0,\n)\n\nembed_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ingestion.py\n\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom model import embed_model\n\ndef create_vectorstore():\n    \"\"\"Create vector store only if it doesn't exist\"\"\"\n    \n    chroma_path = \"./.chroma\"\n    if os.path.exists(chroma_path) and os.listdir(chroma_path):\n        print(\"\ud83d\udcda Loading existing vector store...\")\n        vectorstore = Chroma(\n            collection_name=\"rag-chroma\",\n            embedding_function=embed_model,\n            persist_directory=chroma_path,\n        )\n        return vectorstore\n    \n    print(\"\ud83d\udd04 Creating new vector store...\")\n    \n    urls = [\n        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n    ]\n\n    docs = [WebBaseLoader(url).load() for url in urls]\n    docs_list = [item for sublist in docs for item in sublist]\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n\n    doc_splits = text_splitter.split_documents(docs_list)\n\n    vectorstore = Chroma.from_documents(\n        documents=doc_splits,\n        collection_name=\"rag-chroma\",\n        embedding=embed_model,\n        persist_directory=chroma_path,\n    )\n    \n    print(\"\u2705 Vector store created and persisted!\")\n    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/consts.py\n\nRETRIEVE = \"retrieve\"\nGRADE_DOCUMENTS = \"grade_documents\"\nGENERATE = \"generate\"\nWEBSEARCH = \"websearch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/state.py\n\nfrom typing import List, TypedDict\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        web_search: whether to add search\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    web_search: bool\n    documents: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/nodes/retrieve.py\n\nfrom typing import Any, Dict\nfrom graph.state import GraphState\nfrom ingestion import retriever\n\ndef retrieve(state: GraphState) -> Dict[str, Any]:\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/nodes/grade_documents.py\n\nfrom typing import Any, Dict\n\nfrom graph.chains.retrieval_grader import retrieval_grader\nfrom graph.state import GraphState\n\n\ndef grade_documents(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    filtered_docs = []\n    web_search = False\n    for d in documents:\n        score = retrieval_grader.invoke(\n            {\"question\": question, \"document\": d.page_content}\n        )\n        grade = score.binary_score\n        if grade.lower() == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            web_search = True\n            continue\n    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/nodes/generate.py\n\nfrom typing import Any, Dict\n\nfrom graph.chains.generation import generation_chain\nfrom graph.state import GraphState\n\n\ndef generate(state: GraphState) -> Dict[str, Any]:\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    generation = generation_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/nodes/web_search.py\n\nfrom typing import Any, Dict\nfrom langchain.schema import Document\nfrom langchain_tavily import TavilySearch\nfrom graph.state import GraphState\n\nweb_search_tool = TavilySearch(max_results=3)\n\ndef web_search(state: GraphState) -> Dict[str, Any]:\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n    \n    documents = state.get(\"documents\", [])\n    \n    tavily_results = web_search_tool.invoke({\"query\": question})[\"results\"]\n    joined_tavily_result = \"\\n\".join(\n        [tavily_result[\"content\"] for tavily_result in tavily_results]\n    )\n    web_results = Document(page_content=joined_tavily_result)\n    \n    if documents:\n        documents.append(web_results)\n    else:\n        documents = [web_results]\n    \n    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/chains/router.py\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel, Field\nfrom model import llm_model\n\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"websearch\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\nllm = llm_model\n\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. For all else, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/chains/retrieval_grader.py\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel, Field\nfrom model import llm_model\n\nllm = llm_model\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/chains/generation.py\n\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom model import llm_model\n\nllm = llm_model\n\nprompt = hub.pull(\"rlm/rag-prompt\")\n\ngeneration_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/chains/hallucination_grader.py\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableSequence\nfrom pydantic import BaseModel, Field\nfrom model import llm_model\n\nllm =  llm_model\n\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: bool = Field(\n        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n    )\n\n\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nhallucination_grader: RunnableSequence = hallucination_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/chains/answer_grader.py\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableSequence\nfrom pydantic import BaseModel, Field\nfrom model import llm_model\n\n\nclass GradeAnswer(BaseModel):\n\n    binary_score: bool = Field(\n        description=\"Answer addresses the question, 'yes' or 'no'\"\n    )\n\nllm =  llm_model\n\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nanswer_grader: RunnableSequence = answer_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile graph/graph.py\n\nfrom langgraph.graph import END, StateGraph\nfrom graph.chains.answer_grader import answer_grader\nfrom graph.chains.hallucination_grader import hallucination_grader\nfrom graph.chains.router import RouteQuery, question_router\nfrom graph.consts import GENERATE, GRADE_DOCUMENTS, RETRIEVE, WEBSEARCH\nfrom graph.nodes import generate, grade_documents, retrieve, web_search\nfrom graph.state import GraphState\n\ndef decide_to_generate(state):\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n\n    if state[\"web_search\"]:\n        print(\n            \"---DECISION: NOT ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n        )\n        return WEBSEARCH\n    else:\n        print(\"---DECISION: GENERATE---\")\n        return GENERATE\n\ndef grade_generation_grounded_in_documents_and_question(state: GraphState) -> str:\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": generation}\n    )\n\n    if hallucination_grade := score.binary_score:\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        if answer_grade := score.binary_score:\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n\n\ndef route_question(state: GraphState) -> str:\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n     \n    print(\"---ROUTE QUESTION---\")\n    question = state[\"question\"]\n    source: RouteQuery = question_router.invoke({\"question\": question})\n\n    if source.datasource == WEBSEARCH:\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return WEBSEARCH\n    elif source.datasource == \"vectorstore\":\n        print(\"---ROUTE QUESTION TO RAG---\")\n        return RETRIEVE\n\n\nworkflow = StateGraph(GraphState)\n\nworkflow.add_node(RETRIEVE, retrieve)\nworkflow.add_node(GRADE_DOCUMENTS, grade_documents)\nworkflow.add_node(GENERATE, generate)\nworkflow.add_node(WEBSEARCH, web_search)\n\nworkflow.set_conditional_entry_point(\n    route_question,\n    {\n        WEBSEARCH: WEBSEARCH,\n        RETRIEVE: RETRIEVE,\n    },\n)\n\nworkflow.add_edge(RETRIEVE, GRADE_DOCUMENTS)\nworkflow.add_conditional_edges(\n    GRADE_DOCUMENTS,\n    decide_to_generate,\n    {\n        WEBSEARCH: WEBSEARCH,\n        GENERATE: GENERATE,\n    },\n)\n\nworkflow.add_conditional_edges(\n    GENERATE,\n    grade_generation_grounded_in_documents_and_question,\n    {\n        \"not supported\": GENERATE,\n        \"useful\": END,\n        \"not useful\": WEBSEARCH,\n    },\n)\nworkflow.add_edge(WEBSEARCH, GENERATE)\nworkflow.add_edge(GENERATE, END)\n\napp = workflow.compile()\n\napp.get_graph().draw_mermaid_png(output_file_path=\"graph.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion import create_vectorstore\n# Create the vector store\nvectorstore = create_vectorstore()\n# Create the retriever\nretriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.graph import app\nimport pprint\n\ndef run_rag(question):\n    \"\"\"Helper function to run the RAG graph and print the output.\"\"\"\n    inputs = {\"question\": question}\n    for output in app.stream(inputs):\n        for key, value in output.items():\n            pprint.pprint(f\"Finished running: {key}:\")\n    pprint.pprint(value[\"generation\"])\n\n# Example usage:\nrun_rag(\"What is agent memory?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}